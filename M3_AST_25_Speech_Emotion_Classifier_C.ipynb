{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naru289/Assignment-25/blob/main/M3_AST_25_Speech_Emotion_Classifier_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH3UvBtnW755"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment: Speech and Audio Processing"
      ],
      "id": "hH3UvBtnW755"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXubZhEt6g3u"
      },
      "source": [
        "## Learning Objectives"
      ],
      "id": "KXubZhEt6g3u"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0shlrdB36iZs"
      },
      "source": [
        "At the end of the experiment you will be able to :\n",
        "\n",
        "* extract the features from audio samples/data\n",
        "* implement the Convolutional Neural Networks (CNN) model to classify emotions\n",
        "* evaluate the CNN trained model on the testset"
      ],
      "id": "0shlrdB36iZs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "MFvF4vf7_ngR"
      },
      "id": "MFvF4vf7_ngR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ravdess Dataset**\n",
        "\n",
        "The Ryerson Audio-Visual Database of Emotional Speech and Song [RAVDESS](https://zenodo.org/record/1188976#.YuuUNhxBzIV) contains 1440 files: 60 trials per actor x 24 actors = 1440. The RAVDESS contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes `calm, happy, sad, angry, fearful, surprise`, `and disgust` expressions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. The conditions of the audio files are: 16bit, 48kHz .wav.\n",
        "\n",
        "\n",
        "**File naming convention**\n",
        "\n",
        "Each of the 1440 files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 03-01-06-01-02-01-12.wav). These identifiers define the stimulus characteristics:\n",
        "\n",
        "**Filename identifiers**\n",
        "\n",
        "* Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
        "* Vocal channel (01 = speech, 02 = song).\n",
        "* Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
        "* Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
        "* Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
        "* Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
        "* Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
        "\n",
        "Filename example: `03-01-06-01-02-01-12.wav`\n",
        "\n",
        "    - Audio-only - 03\n",
        "    - Speech - 01\n",
        "    - Fearful - 06\n",
        "    - Normal intensity - 01\n",
        "    - Statement \"dogs\" - 02\n",
        "    - 1st Repetition - 01\n",
        "    - 12th Actor - 12 Female, as the actor ID number is even."
      ],
      "metadata": {
        "id": "gye6FRbM_xRK"
      },
      "id": "gye6FRbM_xRK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIR6a6TvVnRY"
      },
      "source": [
        "## Domain Information"
      ],
      "id": "GIR6a6TvVnRY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Speech Emotion Recognition?**\n",
        "\n",
        "**Speech Emotion Recognition (SER)** is the task of recognizing the emotion from speech, irrespective of the semantics. Humans can efficiently perform this task as a natural part of speech communication, however, the ability to conduct it automatically using programmable devices is a field of active research.\n",
        "\n",
        "Studies of automatic emotion recognition systems aim to create efficient, real-time methods of detecting the emotions of mobile phone users, call center operators and customers, car drivers, pilots, and many other human-machine communication users. Adding emotions to machines forms an important aspect of making machines appear and act in a human-like manner.\n",
        "\n",
        "\n",
        "**What is librosa?**\n",
        "\n",
        "[Librosa](https://librosa.org/doc/latest/index.html) is a Python package, built for speech and audio analytics. It provides modular functions that simplify working with audio data and help in achieving a wide range of applications such as identification of the personal characteristics of different individuals' voice samples, detecting emotions from audio samples etc.\n",
        "\n",
        "For further details on the Librosa package, refer [here](https://conference.scipy.org/proceedings/scipy2015/pdfs/brian_mcfee.pdf).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xbCF9HguHpHY"
      },
      "id": "xbCF9HguHpHY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Steps:"
      ],
      "metadata": {
        "id": "i_QJM07JRvy1"
      },
      "id": "i_QJM07JRvy1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "xWMVQWk58aXm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "cwqosl928dBA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "9l1Iq4QQhk9J"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M3_AST_25_Speech_Emotion_Classifier_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    ipython.magic(\"sx pip install huggingface_hub\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "9l1Iq4QQhk9J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zROXmFsgX10F"
      },
      "source": [
        "### Importing required packages\n"
      ],
      "id": "zROXmFsgX10F"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ce621c0",
      "metadata": {
        "id": "7ce621c0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import librosa\n",
        "from pathlib import Path\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### huggingface_hub library\n",
        "\n",
        "The **huggingface_hub** is a client library to interact with the Hugging Face Hub. The Hugging Face Hub is a platform with over 35K models, 4K datasets, and 2K demos in which people can easily collaborate in their ML workflows. The Hub works as a central place where anyone can share, explore, discover, and experiment with open-source Machine Learning.\n",
        "\n",
        "With huggingface_hub, you can easily download and upload models, extract useful information from the Hub, and do much more. Some example use cases:\n",
        "\n",
        "* Downloading and caching files from a Hub repository.\n",
        "* Creating repositories and uploading an updated model every few epochs.\n",
        "* Extract metadata from all models that match certain criteria (e.g. models for text-classification).\n",
        "* List all files from a specific repository.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "1. Refer to the following [link](https://pypi.org/project/huggingface-hub/) to undertsand about Hugging Face Hub\n",
        "\n",
        "2. Refer to the following [link](https://huggingface.co/docs/hub/models-adding-libraries) to understand about integrating your library with the Hub"
      ],
      "metadata": {
        "id": "UowfUQ9f5vCf"
      },
      "id": "UowfUQ9f5vCf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download datset files from the Hub**\n",
        "\n",
        "The hf_hub_download() function is the main function to download files from the Hub. One advantage of using it is that files are cached locally, so you won't have to download the files multiple times. If there are changes in the repository, the files will be automatically downloaded again.\n",
        "hf_hub_download\n",
        "\n",
        "The function takes the following parameters, downloads the remote file, stores it to disk (in a version-aware way) and returns its local file path.\n",
        "\n",
        "Parameters:\n",
        "\n",
        "    a repo_id (a user or organization name and a repo name, separated by /)\n",
        "    a filename (the name of the file in the repo)\n",
        "    a force_filename (str, optional) — Use this name instead of a generated file name.\n",
        "    a cache_dir which you can specify if you want to control where on disk the files are cached.\n",
        "\n",
        "**Note:** Refer to the following [link](https://huggingface.co/docs/huggingface_hub/package_reference/file_download) to understand more about hub download parameters"
      ],
      "metadata": {
        "id": "ZsTW-u3G--P0"
      },
      "id": "ZsTW-u3G--P0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download the data\n"
      ],
      "metadata": {
        "id": "6OceEtMQLrPv"
      },
      "id": "6OceEtMQLrPv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fd4815a",
      "metadata": {
        "id": "8fd4815a"
      },
      "outputs": [],
      "source": [
        "# Install the Hub library using pip\n",
        "# Integration allows users to download your hosted files directly from the Hub using your library.\n",
        "# Use the hf_hub_download function to retrieve a URL and download files from your repository\n",
        "# Downloaded files are stored in your cache\n",
        "from huggingface_hub import hf_hub_download\n",
        "hf_hub_download(\n",
        "                repo_id='viks66/ravdess_speech',\n",
        "                filename=\"ravdess.zip\",\n",
        "                cache_dir='./',\n",
        "                force_filename='ravdess.zip',\n",
        "                repo_type='dataset',\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbc86524",
      "metadata": {
        "scrolled": true,
        "id": "fbc86524"
      },
      "outputs": [],
      "source": [
        "# Unizip the ravdess dataset files\n",
        "!unzip -q ravdess.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7a721a1",
      "metadata": {
        "id": "b7a721a1"
      },
      "outputs": [],
      "source": [
        "data_path = 'ravdess/'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the ravdess data audio files"
      ],
      "metadata": {
        "id": "s2MP8L-rMWD6"
      },
      "id": "s2MP8L-rMWD6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8df9a2fb",
      "metadata": {
        "id": "8df9a2fb"
      },
      "outputs": [],
      "source": [
        "# Function to get the .wav files and Map the labels for each wav file\n",
        "def get_files(path, extension='.wav'):\n",
        "    return list(path.rglob(f'*{extension}'))\n",
        "label_id = {\"01\": \"neutral\",\n",
        "            '02': \"calm\",\n",
        "            '03': \"happy\",\n",
        "            '04': \"sad\",\n",
        "            '05': \"angry\",\n",
        "            '06': \"fearful\",\n",
        "            '07': \"disgust\",\n",
        "            '08': \"surprised\",\n",
        "           } #let label_id = label_id - 1, eg: 01 = 0, 02=1, etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c426554",
      "metadata": {
        "id": "8c426554"
      },
      "outputs": [],
      "source": [
        "# Get all files from the ravdess data folders\n",
        "all_files = get_files(Path(data_path))\n",
        "print(len(all_files))\n",
        "\n",
        "# storing 24 actors files\n",
        "speakers = set([str(l).split('/')[-2] for l in all_files])\n",
        "\n",
        "# Assigning labels to each .wav file in all the actor folders\n",
        "# Note: the label id starts from 0 to 7\n",
        "labels = {str(l):int(l.stem.split('-')[2][-1])-1 for l in all_files}\n",
        "print(len(speakers))\n",
        "print(speakers)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Play the sample audio of Ravdess data"
      ],
      "metadata": {
        "id": "OoUsqdbx10Cp"
      },
      "id": "OoUsqdbx10Cp"
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display as ipd\n",
        "ipd.Audio('/content/ravdess/Actor_13/03-01-03-02-01-01-13.wav')"
      ],
      "metadata": {
        "id": "hZrEWSAp1ypW"
      },
      "execution_count": null,
      "outputs": [],
      "id": "hZrEWSAp1ypW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICgQxZ-Ab5K9"
      },
      "source": [
        "#### Visualization of a sample audio signal using librosa in time series domain"
      ],
      "id": "ICgQxZ-Ab5K9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "outstanding-caribbean"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import librosa.display\n",
        "\n",
        "sample_audio_path = '/content/ravdess/Actor_05/03-01-02-02-02-02-05.wav'\n",
        "\n",
        "# librosa is used for analyzing and extracting features of an audio signal\n",
        "data, sampling_rate = librosa.load(sample_audio_path)\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# librosa.display.waveshow is used to plot waveform of amplitude vs time\n",
        "librosa.display.waveshow(data, sr=sampling_rate)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "outstanding-caribbean"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fffb7c68",
      "metadata": {
        "id": "fffb7c68"
      },
      "outputs": [],
      "source": [
        "# Slecting the the actors for testing (Actor 01, 02, 03, 04)\n",
        "test_speakers = ['Actor_01', 'Actor_02', 'Actor_03', 'Actor_04']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Emotion Dataset using pytorch and extracting features using MFCC\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BTJLm_f0CFb-"
      },
      "id": "BTJLm_f0CFb-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ2i4WJZdGxw"
      },
      "source": [
        "#### Feature Extraction\n",
        "\n",
        "When we listen to an audio sample it changes constantly. This means that speech is non-stationary signal. Therefore, normal signal processing techniques cannot be applied to get features from audio. However, if the speech signal is observed using a very small duration window, the speech content in that small duration appears to be stationary. That brought in the concept of short-time processing of speech.\n",
        "\n",
        "**MFCC** is a technique for short-time processing of speech.\n",
        "\n",
        "In this short-time processing technique **MFCC**, a small duration window (say 25 milli sec) is considered for processing of the audio samples at a time. This small duration is called a frame. Now, for each of the frames, MFCC features are computed which gives a compact representation of the audio samples.\n",
        "\n",
        "\n",
        "Create **Mel Frequency Cepstral Coefficient (MFCC)** features which represents the short-term power spectrum of a sound and Labels dataset. These feature count is small enough to force us to learn the information of the audio. 12 parameters are related to the amplitude of frequencies. It provides us enough frequency channels to analyze the audio.\n",
        "The output after applying MFCC is a matrix having feature vectors extracted from all the frames\n",
        "\n",
        "Read one WAV file at a time using `Librosa`. An audio time series in the form of a 1-dimensional array for mono or 2-dimensional array for stereo, along with time sampling rate (which defines the length of the array), where the elements within each of the arrays represent the amplitude of the sound waves is returned by `librosa.load()` function.\n",
        "\n",
        "To know more about MFCC, explore the [link](https://jonathan-hui.medium.com/speech-recognition-feature-extraction-mfcc-plp-5455f5a69dd9)"
      ],
      "id": "kZ2i4WJZdGxw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1691f519",
      "metadata": {
        "id": "1691f519"
      },
      "outputs": [],
      "source": [
        "# Prepare the Emotion dataset and split into train, test and validation\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, mode, test_speakers, labels ,num_val=200):\n",
        "        # Select the speakers not in the test test\n",
        "        if mode == 'train' or mode == 'val':\n",
        "            label_names = sorted([l for l in labels if l.split('/')[-2] not in test_speakers])\n",
        "        elif mode == 'test':\n",
        "            label_names = sorted([l for l in labels if l.split('/')[-2] in test_speakers])\n",
        "        if mode == 'val':\n",
        "            label_names = label_names[:num_val]\n",
        "        elif mode == 'train':\n",
        "            label_names = label_names[num_val:]\n",
        "        self.label_names = label_names\n",
        "        self.label_dict = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Function Extracts Features from WAV(aduio) file\"\"\"\n",
        "        # Load an audio file as a floating point time series\n",
        "        # which returns audio time series and sampling rate of y\n",
        "        y, sr = librosa.load(self.label_names[idx])\n",
        "        # Mel-frequency cepstral coefficients (MFCCs), represents the short-term power spectrum of a sound\n",
        "        # number of MFCCs to return is 13 over each time frame\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13).T\n",
        "        return torch.from_numpy(mfcc), self.label_dict[self.label_names[idx]]\n",
        "\n",
        "class BatchPadCollafeFn():\n",
        "     \"\"\" padding sequential data to max length of a batch.\n",
        "     Zero-pads model inputs and targets based on number of frames per step \"\"\"\n",
        "     def __init__(self):\n",
        "        pass\n",
        "     def __call__(self, batch):\n",
        "        # zero-pad all one-hot text sequences to max input length\n",
        "        input_lengths, ids_sorted_decreasing = torch.sort(\n",
        "            torch.LongTensor([len(x[0]) for x in batch]),\n",
        "            dim=0, descending=True)\n",
        "        max_input_len = input_lengths[0]\n",
        "        mfcc_padded = torch.LongTensor(len(batch), max_input_len, batch[ids_sorted_decreasing[0]][0].shape[-1])\n",
        "        mfcc_padded.zero_()\n",
        "        labels = torch.LongTensor(len(batch))\n",
        "        for i in range(len(ids_sorted_decreasing)):\n",
        "            mfcc = batch[ids_sorted_decreasing[i]][0]\n",
        "            mfcc_padded[i, :mfcc.shape[0], :] = mfcc\n",
        "            labels[i] = batch[ids_sorted_decreasing[i]][1]\n",
        "        return mfcc_padded, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the dataset"
      ],
      "metadata": {
        "id": "1vgprM5SEzEu"
      },
      "id": "1vgprM5SEzEu"
    },
    {
      "cell_type": "code",
      "source": [
        "traindataset = EmotionDataset(mode='train', test_speakers=test_speakers, labels=labels)\n",
        "valdataset = EmotionDataset(mode='val', test_speakers=test_speakers, labels=labels)\n",
        "testdataset = EmotionDataset(mode='test', test_speakers=test_speakers, labels=labels)"
      ],
      "metadata": {
        "id": "OWakSMCxEotM"
      },
      "id": "OWakSMCxEotM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0d7aeb4",
      "metadata": {
        "id": "e0d7aeb4"
      },
      "outputs": [],
      "source": [
        "batch_size = 20\n",
        "\n",
        "# A custom collate_fn can be used to customize collation, e.g., padding sequential data to max length of a batch\n",
        "trainloader = DataLoader(traindataset, batch_size=batch_size, collate_fn=BatchPadCollafeFn())\n",
        "valloader = DataLoader(valdataset, batch_size=batch_size, collate_fn=BatchPadCollafeFn())\n",
        "testloader = DataLoader(testdataset, batch_size=batch_size, collate_fn=BatchPadCollafeFn())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the CNN model"
      ],
      "metadata": {
        "id": "OdUOS1d9kb92"
      },
      "id": "OdUOS1d9kb92"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49859799",
      "metadata": {
        "id": "49859799"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, in_channel=13):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channel, 32, 3)\n",
        "        self.conv2 = nn.Conv1d(32, 64, 3)\n",
        "        self.conv3 = nn.Conv1d(64, 128, 3)\n",
        "        self.dense = nn.Linear(128, 8)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = torch.mean(x, -1)\n",
        "\n",
        "        return self.dense(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define train and test functions"
      ],
      "metadata": {
        "id": "xdK_LPAnkh2H"
      },
      "id": "xdK_LPAnkh2H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f51c6d13",
      "metadata": {
        "id": "f51c6d13"
      },
      "outputs": [],
      "source": [
        "def train(loader):\n",
        "    model.train()\n",
        "    n_classes = 8\n",
        "    # Cross entropy as loss function\n",
        "    lossfn = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Define the confusion matrix with zeros of no of classes\n",
        "    confusion_matrix = torch.zeros(n_classes, n_classes)\n",
        "\n",
        "    losses = []\n",
        "    for data, label in tqdm(loader):\n",
        "\n",
        "        # Convert data and labels to torch tensor\n",
        "        data, label = data.to(device), label.to(device)\n",
        "\n",
        "        # Pass the data through the model\n",
        "        out = model(data.float())\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = lossfn(out, label)\n",
        "\n",
        "        # Zero out the gradients\n",
        "        optimiser.zero_grad()\n",
        "\n",
        "        # Do backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights accordingly\n",
        "        optimiser.step()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # Take max probability of predictions\n",
        "        _, preds = torch.max(out, 1)\n",
        "\n",
        "        # Print the loss and updated confusion matrix with labels original and predictions\n",
        "        for t, p in zip(label.view(-1), preds.view(-1)):\n",
        "            confusion_matrix[t.long(), p.long()] += 1\n",
        "    return sum(losses)/len(losses), confusion_matrix.diag()/confusion_matrix.sum(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def val(loader):\n",
        "    # Keep the model to evaluation mode\n",
        "    model.eval()\n",
        "    n_classes = 8\n",
        "    lossfn = nn.CrossEntropyLoss()\n",
        "    confusion_matrix = torch.zeros(n_classes, n_classes)\n",
        "    losses = []\n",
        "    for data, label in tqdm(loader):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        out = model(data.float())\n",
        "        loss = lossfn(out, label)\n",
        "        losses.append(loss.item())\n",
        "        _, preds = torch.max(out, 1)\n",
        "        for t, p in zip(label.view(-1), preds.view(-1)):\n",
        "            confusion_matrix[t.long(), p.long()] += 1\n",
        "    return sum(losses)/len(losses), confusion_matrix.diag()/confusion_matrix.sum(1)"
      ],
      "metadata": {
        "id": "UEJ1qlrLmhlu"
      },
      "id": "UEJ1qlrLmhlu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1cacfdd",
      "metadata": {
        "id": "a1cacfdd"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "lr = 0.0001 # Learning rate\n",
        "model = Model().to(device).float() # Conver the model to cuda runtime\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=lr) # Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "Y-Pm2f7q5STd"
      },
      "id": "Y-Pm2f7q5STd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and validating the model"
      ],
      "metadata": {
        "id": "mJ6RapxuvdyF"
      },
      "id": "mJ6RapxuvdyF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b45f0750",
      "metadata": {
        "scrolled": false,
        "id": "b45f0750"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "trainloss, trainaccs, valloss, valaccs = [], [], [], []\n",
        "for ep in range(num_epochs):\n",
        "    loss, accs = train(trainloader)\n",
        "    trainloss.append(loss)\n",
        "    trainaccs.append(accs)\n",
        "    loss, accs = val(valloader)\n",
        "    valloss.append(loss)\n",
        "    valaccs.append(accs)\n",
        "    print(trainloss[-1], valloss[-1])\n",
        "    print(trainaccs[-1], valaccs[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ],
      "id": "VHfHdGCP_n6Y"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8R6F2KGyiwk",
        "cellView": "form"
      },
      "source": [
        "#@title Q.1. Pre-emphasis processing applies a filter to the input signal that emphasizes the amplitudes of higher frequencies and lowers the amplitudes of lower frequency bands.\n",
        "Answer1 = \"\" #@param [\"\",\"True\", \"False\"]\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "K8R6F2KGyiwk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_J5z0EBJnQl"
      },
      "source": [
        "**Consider the following statements and answer Q2:**\n",
        "\n",
        "i. acoustic feature extraction is performed because raw waveform is high dimensional and difficult to model\n",
        "\n",
        "ii. Acoustic features are extracted to reduce dimensionality\n",
        "\n"
      ],
      "id": "y_J5z0EBJnQl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlhryqWxbXMz",
        "cellView": "form"
      },
      "source": [
        "#@title Q.2. Which of the above statement(s) regarding acoustic feature extraction is true?\n",
        "Answer2 = \"\" #@param [\"\",\"i only\", \"ii only\", \"Both i, ii\", \"Neither i nor ii\"]"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "vlhryqWxbXMz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "NMzKSbLIgFzQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "DjcH1VWSFI2l"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "4VBk_4VTAxCM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "XH91cL1JWH7m"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "z8xLqj7VWIKW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "FzAZHt1zw-Y-"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}